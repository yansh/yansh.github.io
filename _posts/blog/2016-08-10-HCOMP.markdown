---
layout: plain
title: Learning Privacy Expectations by Crowdsourcing Contextual Informational Norms
categories: blog
excerpt: "HCOMP paper"
tags:  Research Paper Crowdsourcing Contextual Integrity
share: true
comments: true
image:
  feature:
date: 2016-08-10
permalink: papers/HCOMP/
---

**by** *Yan Shvartzshnaider, Schrasing Tong, Thomas Wies, Paula Kift, Helen Nissenbaum, Lakshminarayanan Subramanian, Prateek Mittal*


This paper will appear in the proceedings of [the Fourth AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2016)](http://www.humancomputation.com/2016/).

> *NEW* [Freedom to Tinker post about this work.](https://freedom-to-tinker.com/2016/10/31/learning-privacy-expectations-by-crowdsourcing-contextual-informational-norms/)

## Abstract

Designing programmable privacy logic frameworks that correspond
to social, ethical, and legal norms has been a fundamentally
hard problem. Contextual integrity (CI) (Nissenbaum,
2010) offers a model for conceptualizing privacy
that is able to bridge technical design with ethical, legal, and
policy approaches. While CI is capable of capturing the various
components of contextual privacy in theory, it is challenging
to discover and formally express these norms in operational
terms.

In the following, we propose a crowdsourcing method for the
automated discovery of contextual norms. To evaluate the effectiveness
and scalability of our approach, we conducted an
extensive survey on Amazonâ€™s Mechanical Turk (AMT) with
more than 450 participants and 1400 questions. The paper
has three main takeaways: First, we demonstrate the ability
to generate survey questions corresponding to privacy norms
within any context. Second, we show that crowdsourcing enables
the discovery of norms from these questions with strong
majoritarian consensus among users. Finally, we demonstrate
how the norms thus discovered can be encoded into a formal
logic to automatically verify their consistency.

### Datasets

* [Surveys with users responses]({{ site.url }}/resources/surveys.zip): Includes all 16 surveys with individual users' responses.

* [All survey questions]({{ site.url }}/resources/survey_questions.csv): Questions used across all the surveys with associated to them CI parameters.
